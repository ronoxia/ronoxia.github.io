---
layout: page
title: Deep Learning
permalink: /writings/deep-learning
last_modified: 2025-04-08
reading_time: 1 min
date: 2025-04-08
---



This quarter, I’m taking **DS BMED 206: Advanced Machine Learning Applications in Biomedicine**, where we’ll be diving into deep learning through the lens of biomedical problems.

According to the syllabus, we’ll explore:
- Neural networks (Weeks 5–6)
- Sequence models (Week 7)
- Foundation models and transfer learning (Week 8)

While I’ve worked with traditional ML models before, deep learning is still new territory for me. My focus right now is on **understanding architectures** — how they’re built, where they shine, and where they break.

Outside of class, I’ve started exploring:
- *Machine Learning: A Probabilistic Perspective* by Kevin Murphy  
- Selected chapters from *The Deep Learning Book* by Ian Goodfellow  
- Blog tutorials on PyTorch and TensorFlow for hands-on understanding  
- Case studies involving biomedical imaging and genomic sequence classification

### What I’m Thinking About
- What does it mean to apply deep learning in a context where data is high-dimensional, small-N, and full of missingness?
- How do we evaluate a model that performs well statistically but is **hard to interpret** clinically?
- What are the risks of overfitting to *pathology-encoded data* that may reflect systemic bias?

This writing is an open space for notes, insights, and growing clarity — especially as I move into sequence models and foundation models later in the course.

---

### Related
- [Machine Learning](/writings/machine-learning)
- [Signal Processing](/writings/signal-processing)
- [Biomedical Data](/writings/biomedical-data)
